{"id":"07418c73-c72c-4cd4-9235-56ce91c123df","data":{"nodes":[{"data":{"description":"Get chat inputs from the Playground.","display_name":"Chat Input","id":"ChatInput-JsXRJ","node":{"base_classes":["Message"],"beta":false,"conditional_paths":[],"custom_fields":{},"description":"Get chat inputs from the Playground.","display_name":"Chat Input","documentation":"","edited":false,"field_order":["input_value","store_message","sender","sender_name","session_id","files"],"frozen":false,"icon":"MessagesSquare","legacy":false,"lf_version":"1.1.0","metadata":{},"output_types":[],"outputs":[{"cache":true,"display_name":"Message","method":"message_response","name":"message","selected":"Message","types":["Message"],"value":"__UNDEFINED__"}],"pinned":false,"template":{"_type":"Component","background_color":{"_input_type":"MessageTextInput","advanced":true,"display_name":"Background Color","dynamic":false,"info":"The background color of the icon.","input_types":["Message"],"list":false,"load_from_db":false,"name":"background_color","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":""},"chat_icon":{"_input_type":"MessageTextInput","advanced":true,"display_name":"Icon","dynamic":false,"info":"The icon of the message.","input_types":["Message"],"list":false,"load_from_db":false,"name":"chat_icon","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":""},"code":{"advanced":true,"dynamic":true,"fileTypes":[],"file_path":"","info":"","list":false,"load_from_db":false,"multiline":true,"name":"code","password":false,"placeholder":"","required":true,"show":true,"title_case":false,"type":"code","value":"from langflow.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES\nfrom langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, FileInput, MessageTextInput, MultilineInput, Output\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_NAME_USER, MESSAGE_SENDER_USER\n\n\nclass ChatInput(ChatComponent):\n    display_name = \"Chat Input\"\n    description = \"Get chat inputs from the Playground.\"\n    icon = \"MessagesSquare\"\n    name = \"ChatInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            value=\"\",\n            info=\"Message to be passed as input.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_USER,\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_USER,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        FileInput(\n            name=\"files\",\n            display_name=\"Files\",\n            file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,\n            info=\"Files to be sent with the message.\",\n            advanced=True,\n            is_list=True,\n        ),\n        MessageTextInput(\n            name=\"background_color\",\n            display_name=\"Background Color\",\n            info=\"The background color of the icon.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"chat_icon\",\n            display_name=\"Icon\",\n            info=\"The icon of the message.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"text_color\",\n            display_name=\"Text Color\",\n            info=\"The text color of the name\",\n            advanced=True,\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        _background_color = self.background_color\n        _text_color = self.text_color\n        _icon = self.chat_icon\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n            files=self.files,\n            properties={\"background_color\": _background_color, \"text_color\": _text_color, \"icon\": _icon},\n        )\n        if self.session_id and isinstance(message, Message) and self.should_store_message:\n            stored_message = self.send_message(\n                message,\n            )\n            self.message.value = stored_message\n            message = stored_message\n\n        self.status = message\n        return message\n"},"files":{"advanced":true,"display_name":"Files","dynamic":false,"fileTypes":["txt","md","mdx","csv","json","yaml","yml","xml","html","htm","pdf","docx","py","sh","sql","js","ts","tsx","jpg","jpeg","png","bmp","image"],"file_path":"","info":"Files to be sent with the message.","list":true,"name":"files","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"file","value":""},"input_value":{"advanced":false,"display_name":"Text","dynamic":false,"info":"Message to be passed as input.","input_types":["Message"],"list":false,"load_from_db":false,"multiline":true,"name":"input_value","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":"who is the current mayor of Bucaramanga?"},"sender":{"advanced":true,"display_name":"Sender Type","dynamic":false,"info":"Type of sender.","name":"sender","options":["Machine","User"],"placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"str","value":"User"},"sender_name":{"advanced":true,"display_name":"Sender Name","dynamic":false,"info":"Name of the sender.","input_types":["Message"],"list":false,"load_from_db":false,"name":"sender_name","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":"User"},"session_id":{"advanced":true,"display_name":"Session ID","dynamic":false,"info":"The session ID of the chat. If empty, the current session ID parameter will be used.","input_types":["Message"],"list":false,"load_from_db":false,"name":"session_id","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":""},"should_store_message":{"_input_type":"BoolInput","advanced":true,"display_name":"Store Messages","dynamic":false,"info":"Store the message in the history.","list":false,"name":"should_store_message","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"bool","value":true},"text_color":{"_input_type":"MessageTextInput","advanced":true,"display_name":"Text Color","dynamic":false,"info":"The text color of the name","input_types":["Message"],"list":false,"load_from_db":false,"name":"text_color","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":""}}},"type":"ChatInput"},"dragging":false,"height":234,"id":"ChatInput-JsXRJ","position":{"x":-46.08772939507418,"y":694.0585387240216},"positionAbsolute":{"x":-46.08772939507418,"y":694.0585387240216},"selected":false,"type":"genericNode","width":320},{"data":{"description":"Create a prompt template with dynamic variables.","display_name":"Prompt","id":"Prompt-rgjmL","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"template":{"tool_mode":false,"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"{context} -- Prompt from a person that looks for information about Bucaramanga City.\nIn their {question} could request details from Bucaramanga City most recent available information","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput","load_from_db":false},"context":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"context","display_name":"context","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"question":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"question","display_name":"question","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt","documentation":"","custom_fields":{"template":["context","question"]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":null,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true,"required_inputs":null}],"field_order":["template"],"beta":false,"legacy":false,"error":null,"edited":false,"metadata":{},"tool_mode":false,"lf_version":"1.1.0"},"type":"Prompt"},"dragging":false,"height":433,"id":"Prompt-rgjmL","position":{"x":1202.5982355517745,"y":771.8337310560194},"positionAbsolute":{"x":1202.5982355517745,"y":771.8337310560194},"selected":false,"type":"genericNode","width":320},{"data":{"id":"ChatOutput-ikcWr","node":{"base_classes":["Message"],"beta":false,"conditional_paths":[],"custom_fields":{},"description":"Display a chat message in the Playground.","display_name":"Chat Output","documentation":"","edited":false,"field_order":["input_value","should_store_message","sender","sender_name","session_id","data_template","background_color","chat_icon","text_color"],"frozen":false,"icon":"MessagesSquare","legacy":false,"metadata":{},"output_types":[],"outputs":[{"cache":true,"display_name":"Message","method":"message_response","name":"message","selected":"Message","types":["Message"],"value":"__UNDEFINED__"}],"pinned":false,"template":{"_type":"Component","background_color":{"_input_type":"MessageTextInput","advanced":true,"display_name":"Background Color","dynamic":false,"info":"The background color of the icon.","input_types":["Message"],"list":false,"load_from_db":false,"name":"background_color","placeholder":"","required":false,"show":true,"title_case":false,"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":""},"chat_icon":{"_input_type":"MessageTextInput","advanced":true,"display_name":"Icon","dynamic":false,"info":"The icon of the message.","input_types":["Message"],"list":false,"load_from_db":false,"name":"chat_icon","placeholder":"","required":false,"show":true,"title_case":false,"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":""},"code":{"advanced":true,"dynamic":true,"fileTypes":[],"file_path":"","info":"","list":false,"load_from_db":false,"multiline":true,"name":"code","password":false,"placeholder":"","required":true,"show":true,"title_case":false,"type":"code","value":"from langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, MessageInput, MessageTextInput, Output\nfrom langflow.schema.message import Message\nfrom langflow.schema.properties import Source\nfrom langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_NAME_AI, MESSAGE_SENDER_USER\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"MessagesSquare\"\n    name = \"ChatOutput\"\n\n    inputs = [\n        MessageInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_AI,\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_AI,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n        MessageTextInput(\n            name=\"background_color\",\n            display_name=\"Background Color\",\n            info=\"The background color of the icon.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"chat_icon\",\n            display_name=\"Icon\",\n            info=\"The icon of the message.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"text_color\",\n            display_name=\"Text Color\",\n            info=\"The text color of the name\",\n            advanced=True,\n        ),\n    ]\n    outputs = [\n        Output(\n            display_name=\"Message\",\n            name=\"message\",\n            method=\"message_response\",\n        ),\n    ]\n\n    def _build_source(self, _id: str | None, display_name: str | None, source: str | None) -> Source:\n        source_dict = {}\n        if _id:\n            source_dict[\"id\"] = _id\n        if display_name:\n            source_dict[\"display_name\"] = display_name\n        if source:\n            source_dict[\"source\"] = source\n        return Source(**source_dict)\n\n    def message_response(self) -> Message:\n        _source, _icon, _display_name, _source_id = self.get_properties_from_source_component()\n        _background_color = self.background_color\n        _text_color = self.text_color\n        if self.chat_icon:\n            _icon = self.chat_icon\n        message = self.input_value if isinstance(self.input_value, Message) else Message(text=self.input_value)\n        message.sender = self.sender\n        message.sender_name = self.sender_name\n        message.session_id = self.session_id\n        message.flow_id = self.graph.flow_id if hasattr(self, \"graph\") else None\n        message.properties.source = self._build_source(_source_id, _display_name, _source)\n        message.properties.icon = _icon\n        message.properties.background_color = _background_color\n        message.properties.text_color = _text_color\n        if self.session_id and isinstance(message, Message) and self.should_store_message:\n            stored_message = self.send_message(\n                message,\n            )\n            self.message.value = stored_message\n            message = stored_message\n\n        self.status = message\n        return message\n"},"data_template":{"_input_type":"MessageTextInput","advanced":true,"display_name":"Data Template","dynamic":false,"info":"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.","input_types":["Message"],"list":false,"load_from_db":false,"name":"data_template","placeholder":"","required":false,"show":true,"title_case":false,"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":"{text}"},"input_value":{"_input_type":"MessageInput","advanced":false,"display_name":"Text","dynamic":false,"info":"Message to be passed as output.","input_types":["Message"],"list":false,"load_from_db":false,"name":"input_value","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":""},"sender":{"_input_type":"DropdownInput","advanced":true,"combobox":false,"display_name":"Sender Type","dynamic":false,"info":"Type of sender.","name":"sender","options":["Machine","User"],"placeholder":"","required":false,"show":true,"title_case":false,"tool_mode":false,"trace_as_metadata":true,"type":"str","value":"Machine"},"sender_name":{"_input_type":"MessageTextInput","advanced":true,"display_name":"Sender Name","dynamic":false,"info":"Name of the sender.","input_types":["Message"],"list":false,"load_from_db":false,"name":"sender_name","placeholder":"","required":false,"show":true,"title_case":false,"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":"AI"},"session_id":{"_input_type":"MessageTextInput","advanced":true,"display_name":"Session ID","dynamic":false,"info":"The session ID of the chat. If empty, the current session ID parameter will be used.","input_types":["Message"],"list":false,"load_from_db":false,"name":"session_id","placeholder":"","required":false,"show":true,"title_case":false,"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":""},"should_store_message":{"_input_type":"BoolInput","advanced":true,"display_name":"Store Messages","dynamic":false,"info":"Store the message in the history.","list":false,"name":"should_store_message","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"bool","value":true},"text_color":{"_input_type":"MessageTextInput","advanced":true,"display_name":"Text Color","dynamic":false,"info":"The text color of the name","input_types":["Message"],"list":false,"load_from_db":false,"name":"text_color","placeholder":"","required":false,"show":true,"title_case":false,"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":""}},"tool_mode":false,"lf_version":"1.1.0"},"type":"ChatOutput"},"dragging":false,"height":234,"id":"ChatOutput-ikcWr","position":{"x":1991.5531744936707,"y":1058.5326765110171},"positionAbsolute":{"x":1991.5531744936707,"y":1058.5326765110171},"selected":false,"type":"genericNode","width":320},{"id":"Agent-g7NZV","type":"genericNode","position":{"x":1609.2748635945782,"y":822.0292260981729},"data":{"node":{"template":{"_type":"Component","memory":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"memory","value":"","display_name":"External Memory","advanced":true,"input_types":["BaseChatMessageHistory"],"dynamic":false,"info":"Retrieve messages from an external memory. If empty, it will use the Langflow tables.","title_case":false,"type":"other","_input_type":"HandleInput"},"tools":{"trace_as_metadata":true,"list":true,"required":false,"placeholder":"","show":true,"name":"tools","value":"","display_name":"Tools","advanced":false,"input_types":["Tool","BaseTool","StructuredTool"],"dynamic":false,"info":"These are the tools that the agent can use to help with tasks.","title_case":false,"type":"other","_input_type":"HandleInput"},"add_current_date_tool":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"add_current_date_tool","value":true,"display_name":"Add tool Current Date","advanced":true,"dynamic":false,"info":"If true, will add a tool to the agent that returns the current date.","title_case":false,"type":"bool","_input_type":"BoolInput","input_types":[]},"agent_description":{"tool_mode":false,"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"agent_description","value":"A helpful assistant with access to the following tools:","display_name":"Agent Description","advanced":true,"input_types":["Message"],"dynamic":false,"info":"The description of the agent. This is only used when in Tool Mode. Defaults to 'A helpful assistant with access to the following tools:' and tools are added dynamically.","title_case":false,"type":"str","_input_type":"MultilineInput"},"agent_llm":{"tool_mode":false,"trace_as_metadata":true,"options":["Amazon Bedrock","Anthropic","Azure OpenAI","Groq","NVIDIA","OpenAI","Custom"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"agent_llm","value":"NVIDIA","display_name":"Language Model","advanced":false,"input_types":[],"dynamic":false,"info":"","real_time_refresh":true,"title_case":false,"type":"str","_input_type":"DropdownInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langchain_core.tools import StructuredTool\n\nfrom langflow.base.agents.agent import LCToolsAgentComponent\nfrom langflow.base.models.model_input_constants import ALL_PROVIDER_FIELDS, MODEL_PROVIDERS_DICT\nfrom langflow.base.models.model_utils import get_model_name\nfrom langflow.components.helpers import CurrentDateComponent\nfrom langflow.components.helpers.memory import MemoryComponent\nfrom langflow.components.langchain_utilities.tool_calling import ToolCallingAgentComponent\nfrom langflow.io import BoolInput, DropdownInput, MultilineInput, Output\nfrom langflow.schema.dotdict import dotdict\nfrom langflow.schema.message import Message\n\n\ndef set_advanced_true(component_input):\n    component_input.advanced = True\n    return component_input\n\n\nclass AgentComponent(ToolCallingAgentComponent):\n    display_name: str = \"Agent\"\n    description: str = \"Define the agent's instructions, then enter a task to complete using tools.\"\n    icon = \"bot\"\n    beta = False\n    name = \"Agent\"\n\n    memory_inputs = [set_advanced_true(component_input) for component_input in MemoryComponent().inputs]\n\n    inputs = [\n        DropdownInput(\n            name=\"agent_llm\",\n            display_name=\"Model Provider\",\n            info=\"The provider of the language model that the agent will use to generate responses.\",\n            options=[*sorted(MODEL_PROVIDERS_DICT.keys()), \"Custom\"],\n            value=\"OpenAI\",\n            real_time_refresh=True,\n            input_types=[],\n        ),\n        *MODEL_PROVIDERS_DICT[\"OpenAI\"][\"inputs\"],\n        MultilineInput(\n            name=\"system_prompt\",\n            display_name=\"Agent Instructions\",\n            info=\"System Prompt: Initial instructions and context provided to guide the agent's behavior.\",\n            value=\"You are a helpful assistant that can use tools to answer questions and perform tasks.\",\n            advanced=False,\n        ),\n        *LCToolsAgentComponent._base_inputs,\n        *memory_inputs,\n        BoolInput(\n            name=\"add_current_date_tool\",\n            display_name=\"Add tool Current Date\",\n            advanced=True,\n            info=\"If true, will add a tool to the agent that returns the current date.\",\n            value=True,\n        ),\n    ]\n    outputs = [Output(name=\"response\", display_name=\"Response\", method=\"message_response\")]\n\n    async def message_response(self) -> Message:\n        llm_model, display_name = self.get_llm()\n        self.model_name = get_model_name(llm_model, display_name=display_name)\n        if llm_model is None:\n            msg = \"No language model selected\"\n            raise ValueError(msg)\n        self.chat_history = self.get_memory_data()\n\n        if self.add_current_date_tool:\n            if not isinstance(self.tools, list):  # type: ignore[has-type]\n                self.tools = []\n            # Convert CurrentDateComponent to a StructuredTool\n            current_date_tool = CurrentDateComponent().to_toolkit()[0]\n            if isinstance(current_date_tool, StructuredTool):\n                self.tools.append(current_date_tool)\n            else:\n                msg = \"CurrentDateComponent must be converted to a StructuredTool\"\n                raise ValueError(msg)\n\n        if not self.tools:\n            msg = \"Tools are required to run the agent.\"\n            raise ValueError(msg)\n        self.set(\n            llm=llm_model,\n            tools=self.tools,\n            chat_history=self.chat_history,\n            input_value=self.input_value,\n            system_prompt=self.system_prompt,\n        )\n        agent = self.create_agent_runnable()\n        return await self.run_agent(agent)\n\n    def get_memory_data(self):\n        memory_kwargs = {\n            component_input.name: getattr(self, f\"{component_input.name}\") for component_input in self.memory_inputs\n        }\n\n        return MemoryComponent().set(**memory_kwargs).retrieve_messages()\n\n    def get_llm(self):\n        if isinstance(self.agent_llm, str):\n            try:\n                provider_info = MODEL_PROVIDERS_DICT.get(self.agent_llm)\n                if provider_info:\n                    component_class = provider_info.get(\"component_class\")\n                    display_name = component_class.display_name\n                    inputs = provider_info.get(\"inputs\")\n                    prefix = provider_info.get(\"prefix\", \"\")\n                    return self._build_llm_model(component_class, inputs, prefix), display_name\n            except Exception as e:\n                msg = f\"Error building {self.agent_llm} language model\"\n                raise ValueError(msg) from e\n        return self.agent_llm, None\n\n    def _build_llm_model(self, component, inputs, prefix=\"\"):\n        model_kwargs = {input_.name: getattr(self, f\"{prefix}{input_.name}\") for input_ in inputs}\n        return component.set(**model_kwargs).build_model()\n\n    def delete_fields(self, build_config: dotdict, fields: dict | list[str]) -> None:\n        \"\"\"Delete specified fields from build_config.\"\"\"\n        for field in fields:\n            build_config.pop(field, None)\n\n    def update_input_types(self, build_config: dotdict) -> dotdict:\n        \"\"\"Update input types for all fields in build_config.\"\"\"\n        for key, value in build_config.items():\n            if isinstance(value, dict):\n                if value.get(\"input_types\") is None:\n                    build_config[key][\"input_types\"] = []\n            elif hasattr(value, \"input_types\") and value.input_types is None:\n                value.input_types = []\n        return build_config\n\n    def update_build_config(self, build_config: dotdict, field_value: str, field_name: str | None = None) -> dotdict:\n        # Iterate over all providers in the MODEL_PROVIDERS_DICT\n        # Existing logic for updating build_config\n        if field_name == \"agent_llm\":\n            provider_info = MODEL_PROVIDERS_DICT.get(field_value)\n            if provider_info:\n                component_class = provider_info.get(\"component_class\")\n                if component_class and hasattr(component_class, \"update_build_config\"):\n                    # Call the component class's update_build_config method\n                    build_config = component_class.update_build_config(build_config, field_value, field_name)\n\n            provider_configs: dict[str, tuple[dict, list[dict]]] = {\n                provider: (\n                    MODEL_PROVIDERS_DICT[provider][\"fields\"],\n                    [\n                        MODEL_PROVIDERS_DICT[other_provider][\"fields\"]\n                        for other_provider in MODEL_PROVIDERS_DICT\n                        if other_provider != provider\n                    ],\n                )\n                for provider in MODEL_PROVIDERS_DICT\n            }\n            if field_value in provider_configs:\n                fields_to_add, fields_to_delete = provider_configs[field_value]\n\n                # Delete fields from other providers\n                for fields in fields_to_delete:\n                    self.delete_fields(build_config, fields)\n\n                # Add provider-specific fields\n                if field_value == \"OpenAI\" and not any(field in build_config for field in fields_to_add):\n                    build_config.update(fields_to_add)\n                else:\n                    build_config.update(fields_to_add)\n                # Reset input types for agent_llm\n                build_config[\"agent_llm\"][\"input_types\"] = []\n            elif field_value == \"Custom\":\n                # Delete all provider fields\n                self.delete_fields(build_config, ALL_PROVIDER_FIELDS)\n                # Update with custom component\n                custom_component = DropdownInput(\n                    name=\"agent_llm\",\n                    display_name=\"Language Model\",\n                    options=[*sorted(MODEL_PROVIDERS_DICT.keys()), \"Custom\"],\n                    value=\"Custom\",\n                    real_time_refresh=True,\n                    input_types=[\"LanguageModel\"],\n                )\n                build_config.update({\"agent_llm\": custom_component.to_dict()})\n            # Update input types for all fields\n            build_config = self.update_input_types(build_config)\n\n            # Validate required keys\n            default_keys = [\n                \"code\",\n                \"_type\",\n                \"agent_llm\",\n                \"tools\",\n                \"input_value\",\n                \"add_current_date_tool\",\n                \"system_prompt\",\n                \"agent_description\",\n                \"max_iterations\",\n                \"handle_parsing_errors\",\n                \"verbose\",\n            ]\n            missing_keys = [key for key in default_keys if key not in build_config]\n            if missing_keys:\n                msg = f\"Missing required keys in build_config: {missing_keys}\"\n                raise ValueError(msg)\n        if isinstance(self.agent_llm, str) and self.agent_llm in MODEL_PROVIDERS_DICT:\n            provider_info = MODEL_PROVIDERS_DICT.get(self.agent_llm)\n            if provider_info:\n                component_class = provider_info.get(\"component_class\")\n                prefix = provider_info.get(\"prefix\")\n                if component_class and hasattr(component_class, \"update_build_config\"):\n                    # Call each component class's update_build_config method\n                    # remove the prefix from the field_name\n                    if isinstance(field_name, str) and isinstance(prefix, str):\n                        field_name = field_name.replace(prefix, \"\")\n                    build_config = component_class.update_build_config(build_config, field_value, field_name)\n\n        return build_config\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"input_types":[]},"handle_parsing_errors":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"handle_parsing_errors","value":true,"display_name":"Handle Parse Errors","advanced":true,"dynamic":false,"info":"Should the Agent fix errors when reading user input for better processing?","title_case":false,"type":"bool","_input_type":"BoolInput","input_types":[]},"input_value":{"tool_mode":true,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The input provided by the user for the agent to process.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"max_iterations":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"max_iterations","value":15,"display_name":"Max Iterations","advanced":true,"dynamic":false,"info":"The maximum number of attempts the agent can make to complete its task before it stops.","title_case":false,"type":"int","_input_type":"IntInput","input_types":[]},"n_messages":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"n_messages","value":100,"display_name":"Number of Messages","advanced":true,"dynamic":false,"info":"Number of messages to retrieve.","title_case":false,"type":"int","_input_type":"IntInput","input_types":[]},"order":{"tool_mode":false,"trace_as_metadata":true,"options":["Ascending","Descending"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"order","value":"Ascending","display_name":"Order","advanced":true,"dynamic":false,"info":"Order of the messages.","title_case":false,"type":"str","_input_type":"DropdownInput","input_types":[]},"sender":{"tool_mode":false,"trace_as_metadata":true,"options":["Machine","User","Machine and User"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"sender","value":"Machine and User","display_name":"Sender Type","advanced":true,"dynamic":false,"info":"Filter by sender type.","title_case":false,"type":"str","_input_type":"DropdownInput","input_types":[]},"sender_name":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"sender_name","value":"","display_name":"Sender Name","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Filter by sender name.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"session_id":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"session_id","value":"","display_name":"Session ID","advanced":true,"input_types":["Message"],"dynamic":false,"info":"The session ID of the chat. If empty, the current session ID parameter will be used.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"system_prompt":{"tool_mode":false,"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"system_prompt","value":"You will provide clear and neat answers","display_name":"Agent Instructions","advanced":false,"input_types":["Message"],"dynamic":false,"info":"System Prompt: Initial instructions and context provided to guide the agent's behavior.","title_case":false,"type":"str","_input_type":"MultilineInput"},"template":{"tool_mode":false,"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"{sender_name}: {text}","display_name":"Template","advanced":true,"input_types":["Message"],"dynamic":false,"info":"The template to use for formatting the data. It can contain the keys {text}, {sender} or any other key in the message data.","title_case":false,"type":"str","_input_type":"MultilineInput"},"verbose":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"verbose","value":true,"display_name":"Verbose","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"bool","_input_type":"BoolInput","input_types":[]},"max_tokens":{"trace_as_metadata":true,"range_spec":null,"list":false,"required":false,"placeholder":"","show":true,"name":"max_tokens","value":"","display_name":"Max Tokens","advanced":true,"input_types":[],"dynamic":false,"info":"The maximum number of tokens to generate. Set to 0 for unlimited tokens.","real_time_refresh":null,"refresh_button":null,"refresh_button_text":null,"title_case":false,"type":"int","_input_type":"IntInput"},"model_name":{"tool_mode":false,"trace_as_metadata":true,"options":["google/deplot","meta/codellama-70b","meta/llama-3.2-3b-instruct","mistralai/mathstral-7b-v0.1","ibm/granite-3.0-8b-instruct","databricks/dbrx-instruct","thudm/chatglm3-6b","meta/llama-3.2-1b-instruct","google/codegemma-7b","mistralai/codestral-22b-instruct-v0.1","nvidia/llama-3.1-nemotron-51b-instruct","adept/fuyu-8b","zyphra/zamba2-7b-instruct","mistralai/mixtral-8x7b-instruct-v0.1","google/gemma-2b","ibm/granite-8b-code-instruct","rakuten/rakutenai-7b-chat","ai21labs/jamba-1.5-large-instruct","google/recurrentgemma-2b","meta/llama-3.2-90b-vision-instruct","meta/llama-3.1-405b-instruct","google/gemma-2-9b-it","mediatek/breeze-7b-instruct","ibm/granite-3.0-3b-a800m-instruct","mistralai/mistral-large-2-instruct","writer/palmyra-med-70b-32k","meta/llama2-70b","baichuan-inc/baichuan2-13b-chat","abacusai/dracarys-llama-3.1-70b-instruct","ai21labs/jamba-1.5-mini-instruct","institute-of-science-tokyo/llama-3.1-swallow-8b-instruct-v0.1","google/gemma-2-2b-it","meta/llama3-8b-instruct","microsoft/kosmos-2","microsoft/phi-3-mini-128k-instruct","01-ai/yi-large","nvidia/llama3-chatqa-1.5-70b","meta/llama-3.1-70b-instruct","rakuten/rakutenai-7b-instruct","nvidia/nemotron-4-340b-instruct","mistralai/mistral-7b-instruct-v0.2","nvidia/neva-22b","writer/palmyra-fin-70b-32k","microsoft/phi-3-small-8k-instruct","nvidia/nemotron-4-mini-hindi-4b-instruct","writer/palmyra-med-70b","google/paligemma","microsoft/phi-3-mini-4k-instruct","microsoft/phi-3-vision-128k-instruct","nvidia/nemotron-mini-4b-instruct","microsoft/phi-3.5-vision-instruct","institute-of-science-tokyo/llama-3.1-swallow-70b-instruct-v0.1","nvidia/usdcode-llama3-70b-instruct","nvidia/llama-3.1-nemotron-70b-instruct","ibm/granite-34b-code-instruct","mistralai/mamba-codestral-7b-v0.1","nv-mistralai/mistral-nemo-12b-instruct","meta/llama-3.1-8b-instruct","nvidia/vila","mistralai/mistral-large","google/gemma-2-27b-it","nvidia/mistral-nemo-minitron-8b-8k-instruct","microsoft/phi-3-small-128k-instruct","microsoft/phi-3.5-moe-instruct","yentinglin/llama-3-taiwan-70b-instruct","deepseek-ai/deepseek-coder-6.7b-instruct","google/gemma-7b","meta/llama3-70b-instruct","nvidia/llama3-chatqa-1.5-8b","snowflake/arctic","aisingapore/sea-lion-7b-instruct","mistralai/mixtral-8x22b-instruct-v0.1","google/codegemma-1.1-7b","microsoft/phi-3.5-mini-instruct","upstage/solar-10.7b-instruct","tokyotech-llm/llama-3-swallow-70b-instruct-v0.1","meta/llama-3.2-11b-vision-instruct","mistralai/mistral-7b-instruct-v0.3","microsoft/phi-3-medium-128k-instruct","seallms/seallm-7b-v2.5","microsoft/phi-3-medium-4k-instruct","qwen/qwen2-7b-instruct"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"model_name","value":"ibm/granite-8b-code-instruct","display_name":"Model Name","advanced":false,"input_types":[],"dynamic":false,"info":"","real_time_refresh":null,"refresh_button":null,"refresh_button_text":null,"title_case":false,"type":"str","_input_type":"DropdownInput"},"base_url":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"base_url","value":"http://127.0.0.1:8000/v1","display_name":"NVIDIA Base URL","advanced":false,"input_types":[],"dynamic":false,"info":"The base URL of the NVIDIA API. Defaults to https://integrate.api.nvidia.com/v1.","real_time_refresh":null,"refresh_button":true,"refresh_button_text":null,"title_case":false,"type":"str","_input_type":"StrInput"},"nvidia_api_key":{"load_from_db":false,"required":false,"placeholder":"","show":true,"name":"nvidia_api_key","value":"","display_name":"NVIDIA API Key","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The NVIDIA API Key.","real_time_refresh":null,"refresh_button":null,"refresh_button_text":null,"title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"temperature":{"trace_as_metadata":true,"range_spec":null,"list":false,"required":false,"placeholder":"","show":true,"name":"temperature","value":0.1,"display_name":"Temperature","advanced":true,"input_types":[],"dynamic":false,"info":"","real_time_refresh":null,"refresh_button":null,"refresh_button_text":null,"title_case":false,"type":"float","_input_type":"FloatInput"},"seed":{"trace_as_metadata":true,"range_spec":null,"list":false,"required":false,"placeholder":"","show":true,"name":"seed","value":1,"display_name":"Seed","advanced":true,"input_types":[],"dynamic":false,"info":"The seed controls the reproducibility of the job.","real_time_refresh":null,"refresh_button":null,"refresh_button_text":null,"title_case":false,"type":"int","_input_type":"IntInput"},"output_parser":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"output_parser","value":"","display_name":"Output Parser","advanced":true,"input_types":["OutputParser"],"dynamic":false,"info":"The parser to use to parse the output of the model","real_time_refresh":null,"refresh_button":null,"refresh_button_text":null,"title_case":false,"type":"other","_input_type":"HandleInput"}},"description":"Define the agent's instructions, then enter a task to complete using tools.","icon":"bot","base_classes":["Message"],"display_name":"Agent","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"response","hidden":null,"display_name":"Response","method":"message_response","value":"__UNDEFINED__","cache":true,"required_inputs":null}],"field_order":["agent_llm","max_tokens","model_kwargs","json_mode","output_schema","model_name","openai_api_base","api_key","temperature","seed","output_parser","system_prompt","tools","input_value","handle_parsing_errors","verbose","max_iterations","agent_description","memory","sender","sender_name","n_messages","session_id","order","template","add_current_date_tool"],"beta":false,"legacy":false,"edited":false,"metadata":{},"tool_mode":false,"lf_version":"1.1.0"},"type":"Agent","id":"Agent-g7NZV"},"selected":true,"width":320,"height":735,"positionAbsolute":{"x":1609.2748635945782,"y":822.0292260981729},"dragging":false},{"id":"Chroma-I40s4","type":"genericNode","position":{"x":1840.9127392503337,"y":1968.7171404291016},"data":{"node":{"template":{"_type":"Component","embedding":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"embedding","value":"","display_name":"Embedding","advanced":false,"input_types":["Embeddings"],"dynamic":false,"info":"","title_case":false,"type":"other","_input_type":"HandleInput"},"ingest_data":{"tool_mode":false,"trace_as_metadata":true,"list":true,"trace_as_input":true,"required":false,"placeholder":"","show":true,"name":"ingest_data","value":"","display_name":"Ingest Data","advanced":false,"input_types":["Data"],"dynamic":false,"info":"","title_case":false,"type":"other","_input_type":"DataInput"},"allow_duplicates":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"allow_duplicates","value":false,"display_name":"Allow Duplicates","advanced":true,"dynamic":false,"info":"If false, will not add documents that are already in the Vector Store.","title_case":false,"type":"bool","_input_type":"BoolInput"},"chroma_server_cors_allow_origins":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"chroma_server_cors_allow_origins","value":"","display_name":"Server CORS Allow Origins","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"StrInput"},"chroma_server_grpc_port":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"chroma_server_grpc_port","value":"","display_name":"Server gRPC Port","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"int","_input_type":"IntInput"},"chroma_server_host":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"chroma_server_host","value":"","display_name":"Server Host","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"StrInput"},"chroma_server_http_port":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"chroma_server_http_port","value":"","display_name":"Server HTTP Port","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"int","_input_type":"IntInput"},"chroma_server_ssl_enabled":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"chroma_server_ssl_enabled","value":false,"display_name":"Server SSL Enabled","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"bool","_input_type":"BoolInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from copy import deepcopy\n\nfrom chromadb.config import Settings\nfrom langchain_chroma import Chroma\nfrom loguru import logger\n\nfrom langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store\nfrom langflow.base.vectorstores.utils import chroma_collection_to_data\nfrom langflow.io import BoolInput, DataInput, DropdownInput, HandleInput, IntInput, MultilineInput, StrInput\nfrom langflow.schema import Data\n\n\nclass ChromaVectorStoreComponent(LCVectorStoreComponent):\n    \"\"\"Chroma Vector Store with search capabilities.\"\"\"\n\n    display_name: str = \"Chroma DB\"\n    description: str = \"Chroma Vector Store with search capabilities\"\n    documentation = \"https://python.langchain.com/docs/integrations/vectorstores/chroma\"\n    name = \"Chroma\"\n    icon = \"Chroma\"\n\n    inputs = [\n        StrInput(\n            name=\"collection_name\",\n            display_name=\"Collection Name\",\n            value=\"langflow\",\n        ),\n        StrInput(\n            name=\"persist_directory\",\n            display_name=\"Persist Directory\",\n        ),\n        MultilineInput(\n            name=\"search_query\",\n            display_name=\"Search Query\",\n        ),\n        DataInput(\n            name=\"ingest_data\",\n            display_name=\"Ingest Data\",\n            is_list=True,\n        ),\n        HandleInput(name=\"embedding\", display_name=\"Embedding\", input_types=[\"Embeddings\"]),\n        StrInput(\n            name=\"chroma_server_cors_allow_origins\",\n            display_name=\"Server CORS Allow Origins\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"chroma_server_host\",\n            display_name=\"Server Host\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"chroma_server_http_port\",\n            display_name=\"Server HTTP Port\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"chroma_server_grpc_port\",\n            display_name=\"Server gRPC Port\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"chroma_server_ssl_enabled\",\n            display_name=\"Server SSL Enabled\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"allow_duplicates\",\n            display_name=\"Allow Duplicates\",\n            advanced=True,\n            info=\"If false, will not add documents that are already in the Vector Store.\",\n        ),\n        DropdownInput(\n            name=\"search_type\",\n            display_name=\"Search Type\",\n            options=[\"Similarity\", \"MMR\"],\n            value=\"Similarity\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"number_of_results\",\n            display_name=\"Number of Results\",\n            info=\"Number of results to return.\",\n            advanced=True,\n            value=10,\n        ),\n        IntInput(\n            name=\"limit\",\n            display_name=\"Limit\",\n            advanced=True,\n            info=\"Limit the number of records to compare when Allow Duplicates is False.\",\n        ),\n    ]\n\n    @check_cached_vector_store\n    def build_vector_store(self) -> Chroma:\n        \"\"\"Builds the Chroma object.\"\"\"\n        try:\n            from chromadb import Client\n            from langchain_chroma import Chroma\n        except ImportError as e:\n            msg = \"Could not import Chroma integration package. Please install it with `pip install langchain-chroma`.\"\n            raise ImportError(msg) from e\n        # Chroma settings\n        chroma_settings = None\n        client = None\n        if self.chroma_server_host:\n            chroma_settings = Settings(\n                chroma_server_cors_allow_origins=self.chroma_server_cors_allow_origins or [],\n                chroma_server_host=self.chroma_server_host,\n                chroma_server_http_port=self.chroma_server_http_port or None,\n                chroma_server_grpc_port=self.chroma_server_grpc_port or None,\n                chroma_server_ssl_enabled=self.chroma_server_ssl_enabled,\n            )\n            client = Client(settings=chroma_settings)\n\n        # Check persist_directory and expand it if it is a relative path\n        persist_directory = self.resolve_path(self.persist_directory) if self.persist_directory is not None else None\n\n        chroma = Chroma(\n            persist_directory=persist_directory,\n            client=client,\n            embedding_function=self.embedding,\n            collection_name=self.collection_name,\n        )\n\n        self._add_documents_to_vector_store(chroma)\n        self.status = chroma_collection_to_data(chroma.get(limit=self.limit))\n        return chroma\n\n    def _add_documents_to_vector_store(self, vector_store: \"Chroma\") -> None:\n        \"\"\"Adds documents to the Vector Store.\"\"\"\n        if not self.ingest_data:\n            self.status = \"\"\n            return\n\n        _stored_documents_without_id = []\n        if self.allow_duplicates:\n            stored_data = []\n        else:\n            stored_data = chroma_collection_to_data(vector_store.get(limit=self.limit))\n            for value in deepcopy(stored_data):\n                del value.id\n                _stored_documents_without_id.append(value)\n\n        documents = []\n        for _input in self.ingest_data or []:\n            if isinstance(_input, Data):\n                if _input not in _stored_documents_without_id:\n                    documents.append(_input.to_lc_document())\n            else:\n                msg = \"Vector Store Inputs must be Data objects.\"\n                raise TypeError(msg)\n\n        if documents and self.embedding is not None:\n            logger.debug(f\"Adding {len(documents)} documents to the Vector Store.\")\n            vector_store.add_documents(documents)\n        else:\n            logger.debug(\"No documents to add to the Vector Store.\")\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"collection_name":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"collection_name","value":"langflow","display_name":"Collection Name","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"StrInput"},"limit":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"limit","value":"","display_name":"Limit","advanced":true,"dynamic":false,"info":"Limit the number of records to compare when Allow Duplicates is False.","title_case":false,"type":"int","_input_type":"IntInput"},"number_of_results":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"number_of_results","value":10,"display_name":"Number of Results","advanced":true,"dynamic":false,"info":"Number of results to return.","title_case":false,"type":"int","_input_type":"IntInput"},"persist_directory":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"persist_directory","value":"/home/abarbosa/instructlab/chroma-db/","display_name":"Persist Directory","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"StrInput"},"search_query":{"tool_mode":false,"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"search_query","value":"","display_name":"Search Query","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MultilineInput"},"search_type":{"tool_mode":false,"trace_as_metadata":true,"options":["Similarity","MMR"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"search_type","value":"Similarity","display_name":"Search Type","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"DropdownInput"}},"description":"Chroma Vector Store with search capabilities","icon":"Chroma","base_classes":["Data","Retriever"],"display_name":"Chroma DB","documentation":"https://python.langchain.com/docs/integrations/vectorstores/chroma","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Retriever"],"selected":"Retriever","name":"base_retriever","display_name":"Retriever","method":"build_base_retriever","value":"__UNDEFINED__","cache":true,"required_inputs":[],"hidden":true},{"types":["Data"],"selected":"Data","name":"search_results","display_name":"Search Results","method":"search_documents","value":"__UNDEFINED__","cache":true,"required_inputs":[],"hidden":true}],"field_order":["collection_name","persist_directory","search_query","ingest_data","embedding","chroma_server_cors_allow_origins","chroma_server_host","chroma_server_http_port","chroma_server_grpc_port","chroma_server_ssl_enabled","allow_duplicates","search_type","number_of_results","limit"],"beta":false,"legacy":false,"edited":false,"metadata":{},"tool_mode":false,"lf_version":"1.1.0"},"type":"Chroma","id":"Chroma-I40s4"},"selected":false,"width":320,"height":456,"positionAbsolute":{"x":1840.9127392503337,"y":1968.7171404291016},"dragging":false},{"id":"Chroma-bfsF4","type":"genericNode","position":{"x":409.66171114301324,"y":1010.3051976583578},"data":{"node":{"template":{"_type":"Component","embedding":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"embedding","value":"","display_name":"Embedding","advanced":false,"input_types":["Embeddings"],"dynamic":false,"info":"","title_case":false,"type":"other","_input_type":"HandleInput"},"ingest_data":{"tool_mode":false,"trace_as_metadata":true,"list":true,"trace_as_input":true,"required":false,"placeholder":"","show":true,"name":"ingest_data","value":"","display_name":"Ingest Data","advanced":false,"input_types":["Data"],"dynamic":false,"info":"","title_case":false,"type":"other","_input_type":"DataInput"},"allow_duplicates":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"allow_duplicates","value":false,"display_name":"Allow Duplicates","advanced":true,"dynamic":false,"info":"If false, will not add documents that are already in the Vector Store.","title_case":false,"type":"bool","_input_type":"BoolInput"},"chroma_server_cors_allow_origins":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"chroma_server_cors_allow_origins","value":"","display_name":"Server CORS Allow Origins","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"StrInput"},"chroma_server_grpc_port":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"chroma_server_grpc_port","value":"","display_name":"Server gRPC Port","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"int","_input_type":"IntInput"},"chroma_server_host":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"chroma_server_host","value":"","display_name":"Server Host","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"StrInput"},"chroma_server_http_port":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"chroma_server_http_port","value":"","display_name":"Server HTTP Port","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"int","_input_type":"IntInput"},"chroma_server_ssl_enabled":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"chroma_server_ssl_enabled","value":false,"display_name":"Server SSL Enabled","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"bool","_input_type":"BoolInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from copy import deepcopy\n\nfrom chromadb.config import Settings\nfrom langchain_chroma import Chroma\nfrom loguru import logger\n\nfrom langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store\nfrom langflow.base.vectorstores.utils import chroma_collection_to_data\nfrom langflow.io import BoolInput, DataInput, DropdownInput, HandleInput, IntInput, MultilineInput, StrInput\nfrom langflow.schema import Data\n\n\nclass ChromaVectorStoreComponent(LCVectorStoreComponent):\n    \"\"\"Chroma Vector Store with search capabilities.\"\"\"\n\n    display_name: str = \"Chroma DB\"\n    description: str = \"Chroma Vector Store with search capabilities\"\n    documentation = \"https://python.langchain.com/docs/integrations/vectorstores/chroma\"\n    name = \"Chroma\"\n    icon = \"Chroma\"\n\n    inputs = [\n        StrInput(\n            name=\"collection_name\",\n            display_name=\"Collection Name\",\n            value=\"langflow\",\n        ),\n        StrInput(\n            name=\"persist_directory\",\n            display_name=\"Persist Directory\",\n        ),\n        MultilineInput(\n            name=\"search_query\",\n            display_name=\"Search Query\",\n        ),\n        DataInput(\n            name=\"ingest_data\",\n            display_name=\"Ingest Data\",\n            is_list=True,\n        ),\n        HandleInput(name=\"embedding\", display_name=\"Embedding\", input_types=[\"Embeddings\"]),\n        StrInput(\n            name=\"chroma_server_cors_allow_origins\",\n            display_name=\"Server CORS Allow Origins\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"chroma_server_host\",\n            display_name=\"Server Host\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"chroma_server_http_port\",\n            display_name=\"Server HTTP Port\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"chroma_server_grpc_port\",\n            display_name=\"Server gRPC Port\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"chroma_server_ssl_enabled\",\n            display_name=\"Server SSL Enabled\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"allow_duplicates\",\n            display_name=\"Allow Duplicates\",\n            advanced=True,\n            info=\"If false, will not add documents that are already in the Vector Store.\",\n        ),\n        DropdownInput(\n            name=\"search_type\",\n            display_name=\"Search Type\",\n            options=[\"Similarity\", \"MMR\"],\n            value=\"Similarity\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"number_of_results\",\n            display_name=\"Number of Results\",\n            info=\"Number of results to return.\",\n            advanced=True,\n            value=10,\n        ),\n        IntInput(\n            name=\"limit\",\n            display_name=\"Limit\",\n            advanced=True,\n            info=\"Limit the number of records to compare when Allow Duplicates is False.\",\n        ),\n    ]\n\n    @check_cached_vector_store\n    def build_vector_store(self) -> Chroma:\n        \"\"\"Builds the Chroma object.\"\"\"\n        try:\n            from chromadb import Client\n            from langchain_chroma import Chroma\n        except ImportError as e:\n            msg = \"Could not import Chroma integration package. Please install it with `pip install langchain-chroma`.\"\n            raise ImportError(msg) from e\n        # Chroma settings\n        chroma_settings = None\n        client = None\n        if self.chroma_server_host:\n            chroma_settings = Settings(\n                chroma_server_cors_allow_origins=self.chroma_server_cors_allow_origins or [],\n                chroma_server_host=self.chroma_server_host,\n                chroma_server_http_port=self.chroma_server_http_port or None,\n                chroma_server_grpc_port=self.chroma_server_grpc_port or None,\n                chroma_server_ssl_enabled=self.chroma_server_ssl_enabled,\n            )\n            client = Client(settings=chroma_settings)\n\n        # Check persist_directory and expand it if it is a relative path\n        persist_directory = self.resolve_path(self.persist_directory) if self.persist_directory is not None else None\n\n        chroma = Chroma(\n            persist_directory=persist_directory,\n            client=client,\n            embedding_function=self.embedding,\n            collection_name=self.collection_name,\n        )\n\n        self._add_documents_to_vector_store(chroma)\n        self.status = chroma_collection_to_data(chroma.get(limit=self.limit))\n        return chroma\n\n    def _add_documents_to_vector_store(self, vector_store: \"Chroma\") -> None:\n        \"\"\"Adds documents to the Vector Store.\"\"\"\n        if not self.ingest_data:\n            self.status = \"\"\n            return\n\n        _stored_documents_without_id = []\n        if self.allow_duplicates:\n            stored_data = []\n        else:\n            stored_data = chroma_collection_to_data(vector_store.get(limit=self.limit))\n            for value in deepcopy(stored_data):\n                del value.id\n                _stored_documents_without_id.append(value)\n\n        documents = []\n        for _input in self.ingest_data or []:\n            if isinstance(_input, Data):\n                if _input not in _stored_documents_without_id:\n                    documents.append(_input.to_lc_document())\n            else:\n                msg = \"Vector Store Inputs must be Data objects.\"\n                raise TypeError(msg)\n\n        if documents and self.embedding is not None:\n            logger.debug(f\"Adding {len(documents)} documents to the Vector Store.\")\n            vector_store.add_documents(documents)\n        else:\n            logger.debug(\"No documents to add to the Vector Store.\")\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"collection_name":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"collection_name","value":"langflow","display_name":"Collection Name","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"StrInput"},"limit":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"limit","value":"","display_name":"Limit","advanced":true,"dynamic":false,"info":"Limit the number of records to compare when Allow Duplicates is False.","title_case":false,"type":"int","_input_type":"IntInput"},"number_of_results":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"number_of_results","value":10,"display_name":"Number of Results","advanced":true,"dynamic":false,"info":"Number of results to return.","title_case":false,"type":"int","_input_type":"IntInput"},"persist_directory":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"persist_directory","value":"/home/abarbosa/instructlab/chroma-db/","display_name":"Persist Directory","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"StrInput"},"search_query":{"tool_mode":false,"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"search_query","value":"","display_name":"Search Query","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MultilineInput"},"search_type":{"tool_mode":false,"trace_as_metadata":true,"options":["Similarity","MMR"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"search_type","value":"Similarity","display_name":"Search Type","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"DropdownInput"}},"description":"Chroma Vector Store with search capabilities","icon":"Chroma","base_classes":["Data","Retriever"],"display_name":"Chroma DB","documentation":"https://python.langchain.com/docs/integrations/vectorstores/chroma","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Retriever"],"selected":"Retriever","name":"base_retriever","display_name":"Retriever","method":"build_base_retriever","value":"__UNDEFINED__","cache":true,"required_inputs":[],"hidden":true},{"types":["Data"],"selected":"Data","name":"search_results","display_name":"Search Results","method":"search_documents","value":"__UNDEFINED__","cache":true,"required_inputs":[],"hidden":false}],"field_order":["collection_name","persist_directory","search_query","ingest_data","embedding","chroma_server_cors_allow_origins","chroma_server_host","chroma_server_http_port","chroma_server_grpc_port","chroma_server_ssl_enabled","allow_duplicates","search_type","number_of_results","limit"],"beta":false,"legacy":false,"edited":false,"metadata":{},"tool_mode":false,"lf_version":"1.1.0"},"type":"Chroma","id":"Chroma-bfsF4"},"selected":false,"width":320,"height":504,"positionAbsolute":{"x":409.66171114301324,"y":1010.3051976583578},"dragging":false},{"id":"ParseData-8tMP9","type":"genericNode","position":{"x":804.3228048989531,"y":655.1759281492846},"data":{"node":{"template":{"_type":"Component","data":{"tool_mode":false,"trace_as_metadata":true,"list":false,"trace_as_input":true,"required":false,"placeholder":"","show":true,"name":"data","value":"","display_name":"Data","advanced":false,"input_types":["Data"],"dynamic":false,"info":"The data to convert to text.","title_case":false,"type":"other","_input_type":"DataInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.helpers.data import data_to_text\nfrom langflow.io import DataInput, MultilineInput, Output, StrInput\nfrom langflow.schema.message import Message\n\n\nclass ParseDataComponent(Component):\n    display_name = \"Parse Data\"\n    description = \"Convert Data into plain text following a specified template.\"\n    icon = \"braces\"\n    name = \"ParseData\"\n\n    inputs = [\n        DataInput(name=\"data\", display_name=\"Data\", info=\"The data to convert to text.\"),\n        MultilineInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"The template to use for formatting the data. \"\n            \"It can contain the keys {text}, {data} or any other key in the Data.\",\n            value=\"{text}\",\n        ),\n        StrInput(name=\"sep\", display_name=\"Separator\", advanced=True, value=\"\\n\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"parse_data\"),\n    ]\n\n    def parse_data(self) -> Message:\n        data = self.data if isinstance(self.data, list) else [self.data]\n        template = self.template\n\n        result_string = data_to_text(template, data, sep=self.sep)\n        self.status = result_string\n        return Message(text=result_string)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"sep":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"sep","value":"\n","display_name":"Separator","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"StrInput"},"template":{"tool_mode":false,"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"{text}","display_name":"Template","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data.","title_case":false,"type":"str","_input_type":"MultilineInput"}},"description":"Convert Data into plain text following a specified template.","icon":"braces","base_classes":["Message"],"display_name":"Parse Data","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"parse_data","value":"__UNDEFINED__","cache":true}],"field_order":["data","template","sep"],"beta":false,"legacy":false,"edited":false,"metadata":{},"tool_mode":false,"category":"processing","key":"ParseData","lf_version":"1.1.0"},"type":"ParseData","id":"ParseData-8tMP9"},"selected":false,"width":320,"height":302,"positionAbsolute":{"x":804.3228048989531,"y":655.1759281492846},"dragging":false},{"id":"URL-TCJpD","type":"genericNode","position":{"x":545.1042840166748,"y":1860.420908988243},"data":{"node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import re\n\nfrom langchain_community.document_loaders import AsyncHtmlLoader, WebBaseLoader\n\nfrom langflow.custom import Component\nfrom langflow.helpers.data import data_to_text\nfrom langflow.io import DropdownInput, MessageTextInput, Output\nfrom langflow.schema import Data\nfrom langflow.schema.message import Message\n\n\nclass URLComponent(Component):\n    display_name = \"URL\"\n    description = \"Fetch content from one or more URLs.\"\n    icon = \"layout-template\"\n    name = \"URL\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"urls\",\n            display_name=\"URLs\",\n            info=\"Enter one or more URLs, by clicking the '+' button.\",\n            is_list=True,\n            tool_mode=True,\n        ),\n        DropdownInput(\n            name=\"format\",\n            display_name=\"Output format\",\n            info=\"Output format. Use 'Text' to extract the text from the HTML or 'Raw HTML' for the raw HTML content.\",\n            options=[\"Text\", \"Raw HTML\"],\n            value=\"Text\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Data\", name=\"data\", method=\"fetch_content\"),\n        Output(display_name=\"Text\", name=\"text\", method=\"fetch_content_text\"),\n    ]\n\n    def ensure_url(self, string: str) -> str:\n        \"\"\"Ensures the given string is a URL by adding 'http://' if it doesn't start with 'http://' or 'https://'.\n\n        Raises an error if the string is not a valid URL.\n\n        Parameters:\n            string (str): The string to be checked and possibly modified.\n\n        Returns:\n            str: The modified string that is ensured to be a URL.\n\n        Raises:\n            ValueError: If the string is not a valid URL.\n        \"\"\"\n        if not string.startswith((\"http://\", \"https://\")):\n            string = \"http://\" + string\n\n        # Basic URL validation regex\n        url_regex = re.compile(\n            r\"^(https?:\\/\\/)?\"  # optional protocol\n            r\"(www\\.)?\"  # optional www\n            r\"([a-zA-Z0-9.-]+)\"  # domain\n            r\"(\\.[a-zA-Z]{2,})?\"  # top-level domain\n            r\"(:\\d+)?\"  # optional port\n            r\"(\\/[^\\s]*)?$\",  # optional path\n            re.IGNORECASE,\n        )\n\n        if not url_regex.match(string):\n            msg = f\"Invalid URL: {string}\"\n            raise ValueError(msg)\n\n        return string\n\n    def fetch_content(self) -> list[Data]:\n        urls = [self.ensure_url(url.strip()) for url in self.urls if url.strip()]\n        if self.format == \"Raw HTML\":\n            loader = AsyncHtmlLoader(web_path=urls, encoding=\"utf-8\")\n        else:\n            loader = WebBaseLoader(web_paths=urls, encoding=\"utf-8\")\n        docs = loader.load()\n        data = [Data(text=doc.page_content, **doc.metadata) for doc in docs]\n        self.status = data\n        return data\n\n    def fetch_content_text(self) -> Message:\n        data = self.fetch_content()\n\n        result_string = data_to_text(\"{text}\", data)\n        self.status = result_string\n        return Message(text=result_string)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"format":{"tool_mode":false,"trace_as_metadata":true,"options":["Text","Raw HTML"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"format","value":"Text","display_name":"Output format","advanced":false,"dynamic":false,"info":"Output format. Use 'Text' to extract the text from the HTML or 'Raw HTML' for the raw HTML content.","title_case":false,"type":"str","_input_type":"DropdownInput"},"urls":{"tool_mode":true,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":true,"required":false,"placeholder":"","show":true,"name":"urls","value":["https://en.wikipedia.org/wiki/Bucaramanga"],"display_name":"URLs","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Enter one or more URLs, by clicking the '+' button.","title_case":false,"type":"str","_input_type":"MessageTextInput"}},"description":"Fetch content from one or more URLs.","icon":"layout-template","base_classes":["Data","Message"],"display_name":"URL","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Data"],"selected":"Data","name":"data","display_name":"Data","method":"fetch_content","value":"__UNDEFINED__","cache":true},{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"fetch_content_text","value":"__UNDEFINED__","cache":true}],"field_order":["urls","format"],"beta":false,"legacy":false,"edited":false,"metadata":{},"tool_mode":false,"category":"data","key":"URL","lf_version":"1.1.0"},"type":"URL","id":"URL-TCJpD"},"selected":false,"width":320,"height":368,"positionAbsolute":{"x":545.1042840166748,"y":1860.420908988243},"dragging":false},{"id":"RecursiveCharacterTextSplitter-F4qLB","type":"genericNode","position":{"x":1158.0238214439466,"y":1887.1859882522754},"data":{"node":{"template":{"_type":"Component","data_input":{"tool_mode":false,"trace_as_metadata":true,"list":false,"trace_as_input":true,"required":false,"placeholder":"","show":true,"name":"data_input","value":"","display_name":"Input","advanced":false,"input_types":["Document","Data"],"dynamic":false,"info":"The texts to split.","title_case":false,"type":"other","_input_type":"DataInput"},"chunk_overlap":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"chunk_overlap","value":200,"display_name":"Chunk Overlap","advanced":false,"dynamic":false,"info":"The amount of overlap between chunks.","title_case":false,"type":"int","_input_type":"IntInput"},"chunk_size":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"chunk_size","value":500,"display_name":"Chunk Size","advanced":false,"dynamic":false,"info":"The maximum length of each chunk.","title_case":false,"type":"int","_input_type":"IntInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from typing import Any\n\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter, TextSplitter\n\nfrom langflow.base.textsplitters.model import LCTextSplitterComponent\nfrom langflow.inputs.inputs import DataInput, IntInput, MessageTextInput\nfrom langflow.utils.util import unescape_string\n\n\nclass RecursiveCharacterTextSplitterComponent(LCTextSplitterComponent):\n    display_name: str = \"Recursive Character Text Splitter\"\n    description: str = \"Split text trying to keep all related text together.\"\n    documentation: str = \"https://docs.langflow.org/components/text-splitters#recursivecharactertextsplitter\"\n    name = \"RecursiveCharacterTextSplitter\"\n    icon = \"LangChain\"\n\n    inputs = [\n        IntInput(\n            name=\"chunk_size\",\n            display_name=\"Chunk Size\",\n            info=\"The maximum length of each chunk.\",\n            value=1000,\n        ),\n        IntInput(\n            name=\"chunk_overlap\",\n            display_name=\"Chunk Overlap\",\n            info=\"The amount of overlap between chunks.\",\n            value=200,\n        ),\n        DataInput(\n            name=\"data_input\",\n            display_name=\"Input\",\n            info=\"The texts to split.\",\n            input_types=[\"Document\", \"Data\"],\n        ),\n        MessageTextInput(\n            name=\"separators\",\n            display_name=\"Separators\",\n            info='The characters to split on.\\nIf left empty defaults to [\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"].',\n            is_list=True,\n        ),\n    ]\n\n    def get_data_input(self) -> Any:\n        return self.data_input\n\n    def build_text_splitter(self) -> TextSplitter:\n        if not self.separators:\n            separators: list[str] | None = None\n        else:\n            # check if the separators list has escaped characters\n            # if there are escaped characters, unescape them\n            separators = [unescape_string(x) for x in self.separators]\n\n        return RecursiveCharacterTextSplitter(\n            separators=separators,\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n        )\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"separators":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":true,"required":false,"placeholder":"","show":true,"name":"separators","value":"","display_name":"Separators","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The characters to split on.\nIf left empty defaults to [\"\\n\\n\", \"\\n\", \" \", \"\"].","title_case":false,"type":"str","_input_type":"MessageTextInput"}},"description":"Split text trying to keep all related text together.","icon":"LangChain","base_classes":["Data"],"display_name":"Recursive Character Text Splitter","documentation":"https://docs.langflow.org/components/text-splitters#recursivecharactertextsplitter","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Data"],"selected":"Data","name":"data","display_name":"Data","method":"transform_data","value":"__UNDEFINED__","cache":true,"required_inputs":[]}],"field_order":["chunk_size","chunk_overlap","data_input","separators"],"beta":false,"legacy":false,"edited":false,"metadata":{},"tool_mode":false,"category":"langchain_utilities","key":"RecursiveCharacterTextSplitter","lf_version":"1.1.0"},"type":"RecursiveCharacterTextSplitter","id":"RecursiveCharacterTextSplitter-F4qLB"},"selected":false,"width":320,"height":475,"positionAbsolute":{"x":1158.0238214439466,"y":1887.1859882522754},"dragging":false},{"id":"OllamaEmbeddings-yxdTF","type":"genericNode","position":{"x":1265.2111198998107,"y":2439.5323641523055},"data":{"node":{"template":{"_type":"Component","base_url":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"base_url","value":"http://localhost:11434","display_name":"Ollama Base URL","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageTextInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langchain_ollama import OllamaEmbeddings\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import Embeddings\nfrom langflow.io import FloatInput, MessageTextInput, Output\n\n\nclass OllamaEmbeddingsComponent(LCModelComponent):\n    display_name: str = \"Ollama Embeddings\"\n    description: str = \"Generate embeddings using Ollama models.\"\n    documentation = \"https://python.langchain.com/docs/integrations/text_embedding/ollama\"\n    icon = \"Ollama\"\n    name = \"OllamaEmbeddings\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"model\",\n            display_name=\"Ollama Model\",\n            value=\"llama3.1\",\n        ),\n        MessageTextInput(\n            name=\"base_url\",\n            display_name=\"Ollama Base URL\",\n            value=\"http://localhost:11434\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Embeddings\", name=\"embeddings\", method=\"build_embeddings\"),\n    ]\n\n    def build_embeddings(self) -> Embeddings:\n        try:\n            output = OllamaEmbeddings(\n                model=self.model,\n                base_url=self.base_url,\n            )\n        except Exception as e:\n            msg = \"Could not connect to Ollama API.\"\n            raise ValueError(msg) from e\n        return output\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"model":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"model","value":"mxbai-embed-large","display_name":"Ollama Model","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageTextInput"}},"description":"Generate embeddings using Ollama models.","icon":"Ollama","base_classes":["Embeddings"],"display_name":"Ollama Embeddings","documentation":"https://python.langchain.com/docs/integrations/text_embedding/ollama","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Embeddings"],"selected":"Embeddings","name":"embeddings","display_name":"Embeddings","method":"build_embeddings","value":"__UNDEFINED__","cache":true}],"field_order":["model","base_url"],"beta":false,"legacy":false,"edited":true,"metadata":{},"tool_mode":false,"lf_version":"1.1.0"},"type":"OllamaEmbeddings","id":"OllamaEmbeddings-yxdTF"},"selected":false,"width":320,"height":320,"dragging":false,"positionAbsolute":{"x":1265.2111198998107,"y":2439.5323641523055}},{"id":"OllamaEmbeddings-LFO5D","type":"genericNode","position":{"x":-45.28549621250394,"y":1172.365784191551},"data":{"node":{"template":{"_type":"Component","base_url":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"base_url","value":"http://localhost:11434","display_name":"Ollama Base URL","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageTextInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langchain_ollama import OllamaEmbeddings\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import Embeddings\nfrom langflow.io import FloatInput, MessageTextInput, Output\n\n\nclass OllamaEmbeddingsComponent(LCModelComponent):\n    display_name: str = \"Ollama Embeddings\"\n    description: str = \"Generate embeddings using Ollama models.\"\n    documentation = \"https://python.langchain.com/docs/integrations/text_embedding/ollama\"\n    icon = \"Ollama\"\n    name = \"OllamaEmbeddings\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"model\",\n            display_name=\"Ollama Model\",\n            value=\"llama3.1\",\n        ),\n        MessageTextInput(\n            name=\"base_url\",\n            display_name=\"Ollama Base URL\",\n            value=\"http://localhost:11434\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Embeddings\", name=\"embeddings\", method=\"build_embeddings\"),\n    ]\n\n    def build_embeddings(self) -> Embeddings:\n        try:\n            output = OllamaEmbeddings(\n                model=self.model,\n                base_url=self.base_url,\n            )\n        except Exception as e:\n            msg = \"Could not connect to Ollama API.\"\n            raise ValueError(msg) from e\n        return output\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"model":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"model","value":"mxbai-embed-large","display_name":"Ollama Model","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageTextInput"}},"description":"Generate embeddings using Ollama models.","icon":"Ollama","base_classes":["Embeddings"],"display_name":"Ollama Embeddings","documentation":"https://python.langchain.com/docs/integrations/text_embedding/ollama","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Embeddings"],"selected":"Embeddings","name":"embeddings","display_name":"Embeddings","method":"build_embeddings","value":"__UNDEFINED__","cache":true}],"field_order":["model","base_url"],"beta":false,"legacy":false,"edited":true,"metadata":{},"tool_mode":false,"lf_version":"1.1.0"},"type":"OllamaEmbeddings","id":"OllamaEmbeddings-LFO5D"},"selected":false,"width":320,"height":320,"positionAbsolute":{"x":-45.28549621250394,"y":1172.365784191551},"dragging":false}],"edges":[{"source":"Agent-g7NZV","sourceHandle":"{dataType:Agent,id:Agent-g7NZV,name:response,output_types:[Message]}","target":"ChatOutput-ikcWr","targetHandle":"{fieldName:input_value,id:ChatOutput-ikcWr,inputTypes:[Message],type:str}","data":{"targetHandle":{"fieldName":"input_value","id":"ChatOutput-ikcWr","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Agent","id":"Agent-g7NZV","name":"response","output_types":["Message"]}},"id":"reactflow__edge-Agent-g7NZV{dataType:Agent,id:Agent-g7NZV,name:response,output_types:[Message]}-ChatOutput-ikcWr{fieldName:input_value,id:ChatOutput-ikcWr,inputTypes:[Message],type:str}","animated":false,"className":""},{"source":"ChatInput-JsXRJ","sourceHandle":"{dataType:ChatInput,id:ChatInput-JsXRJ,name:message,output_types:[Message]}","target":"Chroma-bfsF4","targetHandle":"{fieldName:search_query,id:Chroma-bfsF4,inputTypes:[Message],type:str}","data":{"targetHandle":{"fieldName":"search_query","id":"Chroma-bfsF4","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"ChatInput","id":"ChatInput-JsXRJ","name":"message","output_types":["Message"]}},"id":"reactflow__edge-ChatInput-JsXRJ{dataType:ChatInput,id:ChatInput-JsXRJ,name:message,output_types:[Message]}-Chroma-bfsF4{fieldName:search_query,id:Chroma-bfsF4,inputTypes:[Message],type:str}","animated":false,"className":""},{"source":"ParseData-8tMP9","sourceHandle":"{dataType:ParseData,id:ParseData-8tMP9,name:text,output_types:[Message]}","target":"Prompt-rgjmL","targetHandle":"{fieldName:context,id:Prompt-rgjmL,inputTypes:[Message,Text],type:str}","data":{"targetHandle":{"fieldName":"context","id":"Prompt-rgjmL","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"ParseData","id":"ParseData-8tMP9","name":"text","output_types":["Message"]}},"id":"reactflow__edge-ParseData-8tMP9{dataType:ParseData,id:ParseData-8tMP9,name:text,output_types:[Message]}-Prompt-rgjmL{fieldName:context,id:Prompt-rgjmL,inputTypes:[Message,Text],type:str}","animated":false,"className":""},{"source":"ChatInput-JsXRJ","sourceHandle":"{dataType:ChatInput,id:ChatInput-JsXRJ,name:message,output_types:[Message]}","target":"Prompt-rgjmL","targetHandle":"{fieldName:question,id:Prompt-rgjmL,inputTypes:[Message,Text],type:str}","data":{"targetHandle":{"fieldName":"question","id":"Prompt-rgjmL","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"ChatInput","id":"ChatInput-JsXRJ","name":"message","output_types":["Message"]}},"id":"reactflow__edge-ChatInput-JsXRJ{dataType:ChatInput,id:ChatInput-JsXRJ,name:message,output_types:[Message]}-Prompt-rgjmL{fieldName:question,id:Prompt-rgjmL,inputTypes:[Message,Text],type:str}","animated":false,"className":"","selected":false},{"source":"Prompt-rgjmL","sourceHandle":"{dataType:Prompt,id:Prompt-rgjmL,name:prompt,output_types:[Message]}","target":"Agent-g7NZV","targetHandle":"{fieldName:input_value,id:Agent-g7NZV,inputTypes:[Message],type:str}","data":{"targetHandle":{"fieldName":"input_value","id":"Agent-g7NZV","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-rgjmL","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-rgjmL{dataType:Prompt,id:Prompt-rgjmL,name:prompt,output_types:[Message]}-Agent-g7NZV{fieldName:input_value,id:Agent-g7NZV,inputTypes:[Message],type:str}","animated":false,"className":""},{"source":"URL-TCJpD","sourceHandle":"{dataType:URL,id:URL-TCJpD,name:data,output_types:[Data]}","target":"RecursiveCharacterTextSplitter-F4qLB","targetHandle":"{fieldName:data_input,id:RecursiveCharacterTextSplitter-F4qLB,inputTypes:[Document,Data],type:other}","data":{"targetHandle":{"fieldName":"data_input","id":"RecursiveCharacterTextSplitter-F4qLB","inputTypes":["Document","Data"],"type":"other"},"sourceHandle":{"dataType":"URL","id":"URL-TCJpD","name":"data","output_types":["Data"]}},"id":"reactflow__edge-URL-TCJpD{dataType:URL,id:URL-TCJpD,name:data,output_types:[Data]}-RecursiveCharacterTextSplitter-F4qLB{fieldName:data_input,id:RecursiveCharacterTextSplitter-F4qLB,inputTypes:[Document,Data],type:other}","animated":false,"className":""},{"source":"RecursiveCharacterTextSplitter-F4qLB","sourceHandle":"{dataType:RecursiveCharacterTextSplitter,id:RecursiveCharacterTextSplitter-F4qLB,name:data,output_types:[Data]}","target":"Chroma-I40s4","targetHandle":"{fieldName:ingest_data,id:Chroma-I40s4,inputTypes:[Data],type:other}","data":{"targetHandle":{"fieldName":"ingest_data","id":"Chroma-I40s4","inputTypes":["Data"],"type":"other"},"sourceHandle":{"dataType":"RecursiveCharacterTextSplitter","id":"RecursiveCharacterTextSplitter-F4qLB","name":"data","output_types":["Data"]}},"id":"reactflow__edge-RecursiveCharacterTextSplitter-F4qLB{dataType:RecursiveCharacterTextSplitter,id:RecursiveCharacterTextSplitter-F4qLB,name:data,output_types:[Data]}-Chroma-I40s4{fieldName:ingest_data,id:Chroma-I40s4,inputTypes:[Data],type:other}","animated":false,"className":""},{"source":"OllamaEmbeddings-yxdTF","sourceHandle":"{dataType:OllamaEmbeddings,id:OllamaEmbeddings-yxdTF,name:embeddings,output_types:[Embeddings]}","target":"Chroma-I40s4","targetHandle":"{fieldName:embedding,id:Chroma-I40s4,inputTypes:[Embeddings],type:other}","data":{"targetHandle":{"fieldName":"embedding","id":"Chroma-I40s4","inputTypes":["Embeddings"],"type":"other"},"sourceHandle":{"dataType":"OllamaEmbeddings","id":"OllamaEmbeddings-yxdTF","name":"embeddings","output_types":["Embeddings"]}},"id":"reactflow__edge-OllamaEmbeddings-yxdTF{dataType:OllamaEmbeddings,id:OllamaEmbeddings-yxdTF,name:embeddings,output_types:[Embeddings]}-Chroma-I40s4{fieldName:embedding,id:Chroma-I40s4,inputTypes:[Embeddings],type:other}","animated":false,"className":""},{"source":"OllamaEmbeddings-LFO5D","sourceHandle":"{dataType:OllamaEmbeddings,id:OllamaEmbeddings-LFO5D,name:embeddings,output_types:[Embeddings]}","target":"Chroma-bfsF4","targetHandle":"{fieldName:embedding,id:Chroma-bfsF4,inputTypes:[Embeddings],type:other}","data":{"targetHandle":{"fieldName":"embedding","id":"Chroma-bfsF4","inputTypes":["Embeddings"],"type":"other"},"sourceHandle":{"dataType":"OllamaEmbeddings","id":"OllamaEmbeddings-LFO5D","name":"embeddings","output_types":["Embeddings"]}},"id":"reactflow__edge-OllamaEmbeddings-LFO5D{dataType:OllamaEmbeddings,id:OllamaEmbeddings-LFO5D,name:embeddings,output_types:[Embeddings]}-Chroma-bfsF4{fieldName:embedding,id:Chroma-bfsF4,inputTypes:[Embeddings],type:other}","animated":false,"className":""},{"source":"Chroma-bfsF4","sourceHandle":"{dataType:Chroma,id:Chroma-bfsF4,name:search_results,output_types:[Data]}","target":"ParseData-8tMP9","targetHandle":"{fieldName:data,id:ParseData-8tMP9,inputTypes:[Data],type:other}","data":{"targetHandle":{"fieldName":"data","id":"ParseData-8tMP9","inputTypes":["Data"],"type":"other"},"sourceHandle":{"dataType":"Chroma","id":"Chroma-bfsF4","name":"search_results","output_types":["Data"]}},"id":"reactflow__edge-Chroma-bfsF4{dataType:Chroma,id:Chroma-bfsF4,name:search_results,output_types:[Data]}-ParseData-8tMP9{fieldName:data,id:ParseData-8tMP9,inputTypes:[Data],type:other}","animated":false,"className":""}],"viewport":{"x":-705.3880866833979,"y":-554.5084309854852,"zoom":0.677759161260404}},"description":"RAG test using an internet URL as a source","name":"BasicRAG-URL","last_tested_version":"1.1.0","endpoint_name":null,"is_component":false}